require(data.table)
require(magrittr)
require(lubridate)
require(broom)
require(tidyr)
require(purrr)
require(dplyr)
require(ggplot2)
require(plotly)



flag_site_data<-function(tmp, sensor_list){
  #' Flag site data
  #'
  #'@description This function flag the dataframe 
  #'with the correct site depending on the sensor id. 
  #'Useful after doing a timeAveraging on the data 
  #'as the factors are dropped
  #'
  #'@param tmp data.frame. The dataframe to flag
  #'@param sensor_list data.frame. sensor_list<-read.csv("C:/Github/AQ-Config/List of sensors per box_lab.csv")
  #'
  #'@return tmp with the additional column "site"
  #'
  tmp$site =""
  for(i in 1:nrow(sensor_list)){
    test<- grepl(sensor_list[i,]$ID,tmp$sensor)
    if(all(!test)){ #no sensor matches
      next
    }
    else{
      tmp[test,]$site <- as.character(sensor_list[i,]$Rpi.Hostname)
    }
    
    
  }
  return(tmp)
  
}


flag_cdt_data<-function(tmp, cdt){
  #' Flag condition data
  #'
  #'@description This function flag the dataframe 
  #'with the correct condition (experiment, source, variaton) depending 
  #'on time. 
  #'Useful after doing a timeAveraging on the data 
  #'as the factors are dropped
  #'
  #'@param tmp data.frame. The dataframe to flag
  #'@param cdt data.frame. "C:/Data/LabStudy/conditions.Rds", generated by "validate_lab_data.R"
  #'
  #'@return tmp with the additional columns "exp", "source", "variation"
  #'
  
  tmp$exp = ""
  tmp$source = ""
  tmp$variation = ""
  
  for(i in 1:nrow(cdt)){
    tmp[tmp$date>=as.POSIXct(cdt[i,]$start.date,tz="UTC") &
          tmp$date<=as.POSIXct(cdt[i,]$end.date,tz="UTC"),] %<>% mutate(exp = as.character(cdt[i,]$exp),
                                                                        source = as.character(cdt[i,]$source),
                                                                        variation = as.character(cdt[i,]$variation))
  }
  return(tmp)
}



delay_and_merge <- function(df_sensors, dusttrak, delay){
  df_sensors %<>%
    mutate(date=ymd_hms(date)-seconds(delay))
  
  df_sensors %>%
    group_by(date = make_datetime(year =  year(date), month = month(date), 
                                  day = day(date),hour = hour(date), 
                                  min=minute(date), sec = round(second(date)/10,0)*10),
             sensor)%>% 
    summarise_each(funs(mean(.,na.rm=TRUE)),PM25) ->tmp
  
  tmp<-flag_cdt_data(tmp,cdt)
  tmp<-flag_site_data(tmp,sensor_list)
  merged <- inner_join(tmp,select(dusttrak,pm2.5,date),by=c("date","date"))
}




sum_absolute_difference_per_sensor<-function(merged, delay){
  merged %>%
    mutate(diff = abs(PM25-pm2.5)) %>%
    group_by(sensor) %>%
    summarise(sum_diff=sum(diff,na.rm = TRUE)) %>%
    mutate(delay=delay)
  
}

clean_regression<-function(df){
  lm(df$pm2.5~df$PM25) %>% broom::tidy()
}

rsquare<-function(merged,delay){
  
 df_sensors %>%
    group_by(sensor.x) %>%
    nest()->tmp
  tmp %>%
    mutate(regression = map(data,clean_regression)) %>%
      unnest(regression, .drop=TRUE)
    
  
}


finite.differences <- function(x, y) {
  #' Finite differences
  #' Calculate the differentiate of f(x) = y
  #' 
  #' x and y must have the same length
  #' 
  #' @param x numeric vector
  #' @param y numeric vector
  #'
  #' @return fdx - numeric vector of length = length(x)
  #' @export
  #'
  #' @examples finite.differences(as.numeric(template$date), template$pm2.5)
  if (length(x) != length(y)) {
    stop('x and y vectors must have equal length')
  }
  
  n <- length(x)
  
  # Initialize a vector of length n to enter the derivative approximations
  fdx <- vector(length = n)
  
  # Iterate through the values using the forward differencing method
  for (i in 2:n) {
    fdx[i-1] <- (y[i-1] - y[i]) / (x[i-1] - x[i])
  }
  
  # For the last value, since we are unable to perform the forward differencing method 
  # as only the first n values are known, we use the backward differencing approach
  # instead. Note this will essentially give the same value as the last iteration 
  # in the forward differencing method, but it is used as an approximation as we 
  # don't have any more information
  fdx[n] <- (y[n] - y[n - 1]) / (x[n] - x[n - 1])
  
  return(fdx)
}


round2 = function(x, n) {
  posneg = sign(x)
  z = abs(x)*10^n
  z = z + 0.5
  z = trunc(z)
  z = z/10^n
  z*posneg
}


prepare_sensor_data <- function(sensor_file, cdt = cdt){
  #' Prepare the data from the sensors and the DustTrak by 
  #' sampling it every 10 seconds
  #' 
  #' 
  #' Flags the dataset
  #'
  #' @param sensor_file 
  #' @param dusttrak_file 
  #'
  #' @return
  #' @export
  #'
  #' @examples
  df <- readRDS(file = sensor_file)
  
  df %>%
    group_by(sensor, date = make_datetime(year =  year(date), month = month(date), day = day(date),hour = hour(date), min=minute(date), sec = round2(second(date)/10,0)*10))%>% 
    summarise_each(funs(mean(., na.rm = TRUE)), PM25) -> df_avg
  
  df_avg<-flag_cdt_data(df_avg, cdt)
  df_avg<-flag_site_data(df_avg, sensor_list)
  
  
  df_avg
}


prepare_dusttrak_data <- function(dusttrak_file, cdt = cdt){
  #' Prepare the data from the DustTrak by averaging it 
  #' every 10 seconds
  #' 
  #'Replace -1 by 0 for the DustTrak readings and applies
  #'a coefficient 1000 to pm2.5 to convert from mg/m3 to
  #'ug/m3.
  #' @param dusttrak_file 
  #'
  #' @return
  #' @export
  #'
  #' @examples
  dusttrak <- readRDS(file = dusttrak_file) %>%
    mutate(date = date+seconds(5))
  dusttrak %>%
    group_by(date = make_datetime(year =  year(date), month = month(date), day = day(date),hour = hour(date), min=minute(date), sec = round2(second(date)/10,0)*10))%>% 
    summarise_each(funs(mean(., na.rm = TRUE)), pm2.5) ->dt_avg
  
  
  
  dt_avg$sensor <- "DustTrak"
  dt_avg$pm2.5 <- dt_avg$pm2.5*1000
  dt_avg[dt_avg$pm2.5<0,]$pm2.5 <- 0 # Replace the -1 by zero.
  dt_avg <- flag_cdt_data(dt_avg, cdt)
  dt_avg$site <- "Lab"
  dt_avg
  
}

TLS <- function(df, dusttrak, threshold=0){
  #print(df)
  if(is.null(df)){
    warning("dimension of the subset is 0")
    return(tibble(slope = NA, intercept=NA, n=0))
  }
  start.date<- min(df$date)
  end.date<-max(df$date)
  # df <- df %>%
  #   timeAverage(pollutant="PM25", 
  #               avg.time = "10 sec", start.date = start.date, fill=TRUE)
  # dusttrak<-dusttrak %>%
  #   filter(date>=start.date,date<=end.date) %>%
  #   timeAverage(pollutant="pm2.5", 
  #               avg.time = "10 sec", start.date = start.date, fill=TRUE)
  df <- df %>%
    inner_join(dusttrak, by= c("date", "date")) %>%
    filter(pm2.5>=threshold)
  if(is.null(df)){
    warning("inner join dimension is 0")
    return(tibble(slope = NA, intercept=NA, n=0))
  }
  v<-prcomp(na.omit(cbind(df$pm2.5,df$PM25)))$rotation
  slope <- v[2,1]/v[1,1]
  intercept<- mean(df$PM25,na.rm=TRUE)-mean(dusttrak$pm2.5,na.rm=TRUE)*slope
  
  return(tibble(slope = slope, intercept=intercept, n=nrow(df)))
}



clean_regression<-function(df, dusttrak, threshold=0){
  #print(df)
  #print(df$exp)
  if(is.null(df)){
    warning("inner join dimension is 0")
    return(tibble(slope = NA, slope_std = NA, slope_p_value = NA,
                  intercept = NA, intercept_std = NA, intercept_p_value = NA,
                  r_squared_adjusted = NA))
  }
  df <- df %>%
    filter(PM25 >= threshold) %>%
    inner_join(dusttrak, by= c("date", "date"))
  if(is.null(df)){
    warning("inner join dimension is 0")
    return(tibble(slope = NA, slope_std = NA, slope_p_value = NA,
                  intercept = NA, intercept_std = NA, intercept_p_value = NA,
                  r_squared_adjusted = NA))
  }
  
 res<-lm(df$PM25~df$pm2.5) 
 
 stats <-res %>% broom::glance()
 coeffs <- res %>% broom::tidy()
  spearman <- cor(df$PM25, df$pm2.5, use = "pairwise", method = "spearman")   
   return(tibble(slope = coeffs[2,]$estimate, slope_std =coeffs[2,]$std.error, slope_p_value =coeffs[2,]$p.value,
                 intercept = coeffs[1,]$estimate, intercept_std =coeffs[1,]$std.error, intercept_p_value =coeffs[1,]$p.value,
                 r_squared_adjusted = stats[1,]$adj.r.squared, r_squared = stats[1,]$r.squared, 
                 r_squared_p_value =stats[1,]$p.value,  spearman = spearman))
 
}



  